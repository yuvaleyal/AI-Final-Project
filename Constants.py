BOARD_SIZE = 8
BLACK = 1
BLACK_QUEEN = 10
WHITE = -1
WHITE_QUEEN = -10
QUEEN_MULTIPLIER = 10
EMPTY = 0
TIE = 0
NOT_OVER_YET = 2
CMD = [False]
OBJECTS_DIR = 'objects'
Q_Learning_OB_PATH = lambda player: f'{OBJECTS_DIR}/{player}_Q_LearningAgent.pkl'
DQN_OB_PATH = lambda player: f'{OBJECTS_DIR}/{player}_DQN_NET.pth'
AlphaZeroNET_OB_PATH = lambda player: f'{OBJECTS_DIR}/{player}_AlphaZeroNET.pth'
MCTS_OB_PATH = lambda player: f'{OBJECTS_DIR}/{player}_MCTS.pth'

PLAYER_NAME_A = "pa"
PLAYER_NAME_B = "pb"
HUMAN = 'human'
AlphaZero = 'alphazero'
ACTION_SIZE = 32 * 32
EVAL_MODE = "eval"
TRAINING_MODE = "train"
TESTING_MODE = "test"
MODE = [EVAL_MODE]
PROG_MODE = [TRAINING_MODE, EVAL_MODE, TESTING_MODE]
DQN = 'dqn'
TYPE_PLAYERS = ['random', HUMAN, 'minimax', 'rl', DQN, 'first_choice', AlphaZero]
ADVANCED_PLAYERS = []
UCT_C = 1.41
NUM_SIMULATIONS = 800
DIRICHLET_ALPHA = 0.3
DIRICHLET_EPSILON = 0.25
NUM_CHANNELS = 256
NUM_RESIDUAL_BLOCKS = 19
BATCH_SIZE = 2048
EPOCHS = 20
LEARNING_RATE = 0.001
WEIGHT_DECAY = 0.0001
